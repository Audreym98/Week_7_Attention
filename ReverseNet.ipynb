{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84dfe4de",
   "metadata": {},
   "source": [
    "# Week 7: ReverseNet\n",
    "### (A very silly but pedagogically useful attention network)\n",
    "\n",
    "Transformers are sequence-to-sequence models, primarily designed to operate on complex datasets in natural language processing, computer vision, or more exotic applications.  However, we can conceptualize much simpler sequence-to-sequence tasks.  For example, one very simple sequence-to-sequence task is to simply reverse the order of the input sequence.  This turns out to be a bit difficult for a variety of different architectures: first, we can presume that we'll have variable length sequences, and so a multilayer perceptron will not work for this straightaway.  This turns out to be somewhat challenging for RNNs as well, because the entire sequence has to be stored in the RNN's memory, and then recalled in reverse order.  (This is an obvious case where \"normal\" computing has the edge over machine learning: this task is easily done with either arrays or stacks).\n",
    "\n",
    "However, this is a task that we can learn to do with attention networks (I am loathe to call these transformers because what we will be doing is much simpler).  Indeed, it is easy to see precisely what behavior we would like attention to do: the first position in the output sequence should attend to the last position in the input sequence, the second to the second to last, and so on.  Can we learn to do this simply from paired input and output sequences?  \n",
    "\n",
    "## Sequences to reverse\n",
    "What sequences shall we apply this to?  One very simple case would be to reverse 1D real valued functions.  To that end, we can create a method that randomly generates a 1D real valued function of random length (in this case from a Gaussian Process), as well as its inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d14a85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self):\n",
    "        pass    \n",
    "    \n",
    "    def __call__(self,n_in):\n",
    "        x = torch.linspace(0,n_in-1,n_in)\n",
    "        D = torch.cdist(x.view(-1,1),x.view(-1,1))\n",
    "        l = np.random.randn()\n",
    "        K = torch.exp(-(D/l)**2)\n",
    "        L = torch.cholesky(K + 1e-5*torch.eye(K.shape[0]))\n",
    "        v = L @ torch.randn(L.shape[1])\n",
    "        v_reverse = torch.from_numpy(v.numpy()[::-1].copy())\n",
    "        return v,v_reverse\n",
    "\n",
    "generator = DataGenerator()\n",
    "v,v_reverse = generator(15)\n",
    "\n",
    "plt.plot(v,'k-')\n",
    "plt.plot(v_reverse,'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f89cc",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Again, we would like to train a neural network to take as input the variable length sequence $\\vec{v}_i$ and return its reversal.  \n",
    "\n",
    "Because these functions are random, we would expect that the actual function values themselves will not be very informative for the purposes of reordering: there's not really a correlation between the function value and where it appears in the domain.  As such, we won't view the function values themselves as features as we would for a proper transformer.  Instead, we'll rely solely on the *positional encoding* to act as our feature.  \n",
    "\n",
    "There are many versions of plausible positional encodings.  A popular choice in the context of transformers is to encode the position through applying sinusoids of varying frequencies to the sequence position.  This is roughly equivalent to encoding the sequence index in binary.  Alternatively, positional encodings can be learned.  However, for our purposes it will be useful to use the simple encoding scheme of *index normalized by sequence length*\n",
    "$$\n",
    "\\vec{x}_\\text{Enc,i} = \\frac{i}{d_{in}},\n",
    "$$\n",
    "where $i$ is the sequence index.  The resulting encoding vector is zero at the zero-th position and one at the last.  Note that this choice throws away the objective notion of distance, which one could imagine to be useful for more complicated language modelling tasks where that actual objective distance might be an important indicator of, say, how related two words are.  In our task that distance is not important, and the simple encoding of relative position that the above equation affords works very well.  **Write a function that takes a sequence length $n_{in}$ and produces the positional encoding.  (This should be basically a one-liner)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12412d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(n_in):\n",
    "    return None # Implement positional encoding in range [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb47979",
   "metadata": {},
   "source": [
    "### Important note\n",
    "One effect of only using positional encodings as features (and not the function values themselves) is that we can generate the output sequences all at once: there's no need to build sequences one output at a time, because that output will not be used as input by the next sequence element.  \n",
    "\n",
    "\n",
    "## Key and query networks\n",
    "Let's begin to think about network architecture.  Again, our goal is to take an input sequence, augment it with a positional encoding and output the reversed sequence.  Let's hypothesize that this can be done \n",
    "as follows:\n",
    "$$\n",
    "\\vec{v}_{reversed} = \\text{Softmax}\\left(\\frac{Q K^T}{d_k}\\right) \\vec{v}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "Q = q(\\vec{x}_\\text{Enc}) \\in \\mathbb{R}^{d_{in} \\times d_k}\n",
    "$$\n",
    "$$\n",
    "K = k(\\vec{x}_\\text{Enc}) \\in \\mathbb{R}^{d_{in} \\times d_k}.\n",
    "$$\n",
    "The functions $q(\\cdot)$ and $k(\\cdot)$ are neural networks that act in a position-wise fashion: they should accept as input a feature and output a query or key vector at that position.  Applying this function to every element in the sequence produces $Q$ and $K$.  Again, we're not using $\\vec{v}$ as a feature, so these can simply act on the positional encodings.  \n",
    "\n",
    "**Create both a key and query function.  These can be quite simple: perhaps an MLP with one hidden layer that accepts a sequence of positional encodings and outputs a matrix of size $d_{in} \\times d_k$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyNet(torch.nn.Module):\n",
    "    def __init__(self,d_positional_encoding,d_k):\n",
    "        super(KeyNet, self).__init__()\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return None\n",
    "    \n",
    "class QueryNet(torch.nn.Module):\n",
    "    def __init__(self,d_positional_encoding,d_k):\n",
    "        super(QueryNet, self).__init__()\n",
    "       \n",
    "    def forward(self,X):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f951f008",
   "metadata": {},
   "source": [
    "## The ReverseNet\n",
    "Now that we have a means of generating keys and queries, all we need to do is to implement the attention function.  **Implement a torch module that**\n",
    "- takes as input a variable length sequence\n",
    "- computes the positional encoding of input and output sequence (are they different?) \n",
    "- uses that encoding to produce a key matrix for the input sequence and a query matrix for the output sequence\n",
    "- computes the scaled dot-product attention matrix\n",
    "- applies the attention matrix to the input sequence to produce the output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reverser(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # This init method should initialize key and query networks\n",
    "        super(Reverser, self).__init__()\n",
    "        \n",
    "    def forward(self,V):\n",
    "        # generate positional encodings\n",
    "        # compute query and key matrices\n",
    "        # compute attention matrix\n",
    "        # produce output sequence\n",
    "        return None\n",
    "    \n",
    "reverser = Reverser() # You may wish to add arguments to the constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3341e24",
   "metadata": {},
   "source": [
    "## Training\n",
    "Of course this network will not yet be skillful at reversing sequences because it hasn't been trained.  We can implement a simple training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(reverser.parameters(),lr=1e-2)\n",
    "batch_size = 25\n",
    "n_epochs = 1000\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0.0\n",
    "    for j in range(batch_size):\n",
    "        d_in = np.random.randint(20,30)\n",
    "        v,v_reverse = generator(d_in)\n",
    "\n",
    "        v_reverse_predicted = reverser(v)\n",
    "        loss += (((v_reverse_predicted - v_reverse )**2).mean())\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%50==0:\n",
    "        print(f\"Iteration: {i}, Loss: {loss.item():.02f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca8cd3",
   "metadata": {},
   "source": [
    "Having successfully trained the network, we can look at some output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "v,_ = generator(20)\n",
    "v_r = reverser(v)\n",
    "plt.plot(v,'k-')\n",
    "plt.plot(v_r.detach(),'r--')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e23f6",
   "metadata": {},
   "source": [
    "Wow!  We have managed to turn a simple numpy call into a rather complicated neural network, but it works.  However, despite the simplicity of the task, this is perhaps useful for checking your intuition regarding how attention works.  In particular, what do you suppose the attention matrix \n",
    "$$\n",
    "A = \\text{Softmax}\\left(\\frac{Q K^T}{d_k}\\right)\n",
    "$$\n",
    "looks like for this problem?  **Spend a moment talking to your neighbor or making a sketch**.  **Then modify the Reverser class' forward method to output not not only the reversed sequence, but also the attention matrix.  Plot that matrix (plt.imshow works well for this) for a few examples and see if this aligns with your intuition.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da31ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
